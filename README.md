# Deep_Learning_PUC

Here i uploaded my homework and projects from IIC3697 Deep Learning course at PUC, taught by professor √Ålvaro Soto.

We used mostly Pytorch, algouth some of the experimends were done in keras.

The content of this repo is as follows:

**T1:** We used Keras to build CNNs fort image classification. The dataset used consisted in images of animals. We compared popuplar architechtures at the time for such task: ResNet and VGG19. We were interested in comparing accuracy, loss, number of parameters, the effect of choosing different layers ( max pooling, zero padding, ext), batch size, and the cases in which the nets performed poorly.

**T2:** Here i explored Finetunning and the effect of several activation functions in learning. I used Pytorch on a GPU and implemented a ResNet architechture on trhe birds dataset. The first experiment consisted in training the model from scratch, and observing that the performance was bad. The, we used transfer learning by using ImageNet's pretrained weights, and then fine-tunned the model, achieving 84% validation acuracy. We then compared both models, by observintg the best performing and worst performing clases. Finally, we tested and implemented into the Net two  interesting loss functions : Triplet Loss and Focal Loss on a Face dataset.

**T3:** This Notebook is dedicated to experimentation with RNNs. First, i worked in using DL to predict airline pricesusing a simple RNN nmetwork with RNN and linear layers, and then improving the results with an LSTM layer. We explored the effect of different configurations for the length of the sequences. Then, I implemented a Neural Machine Translation Network. I used a seq-2-seq model to translate english to spanish. I prepared the embeddings using a tokenization of the words and then pretrained embeddings. The encoder and decoder were implemented from the "Attention is all you need" paper, with particular care for numerical efficency, since the workload on the GPU was high (I used Google Colab, and faced RAM shortages whenever a computation was not carefully monitored and executed). I evaluated the error by using the Bleu Score, common in NLP translation tasks.

**Project:** Very similar to T3, I implemented a Neural Machine Content Summarizer Network. The objective was to train a model to summarize english text into smaller sentences. The idea was to focus not necesarilly in sentence structurte,  but rather in content.



I have a lot to learn about NNN. This experience of experimentation gave me great insight about the challenges and posibilities of DL in the future. Surely, I will continue working on DL-related problems.



